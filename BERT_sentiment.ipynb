{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenoCarrizo/Proyecto_Titulo/blob/master/BERT_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tadN4hSCP9p"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se usara BERT para crear una capa de embedding.\n",
        "El dataset y el modelo seran los mismo. La diferencia estara en que la red neuronal convolucional, en vez de utilizar la capa de embedding creada por nosotros usando tensorflow, se usara la capa de embedding de BERT, de modo que no vamos a entrenar los pesos de las variables que se corresponden a la capa de embedding, sino que sera BERT quien las maneje por nosotros. En pocas palabras, se obtendra una representacion vectorial de nuestras palabras optimizada por BERT y se utilizara para ver si nuestro modelo mejora con respecto a como lo teniamos en la seccion precedente.\n",
        "Esto de puede usar en cualquier proyecto de NLP, usar el la capa de embedding de BERT.\n",
        "\n",
        "Lo que va a cambiar va a ser una parte de la limpieza (preprosesado), ya que habra que codificar la frase del modo que espera la capa de embedding y por supuesto, el entrenamiento y la evaluacion va a ser el mismo.\n",
        "\n",
        "Hay dos cosas grandes que hay que hacer para adaptar el modelo. La primera es la entrada, porque la primera de las capas de nuestro modelo ahora va a ser una capa BERT.\n",
        "\n",
        "En la creacion del modelo, hay que cambiar la capa de embedding por defecto, para usar la capa de embedding de BERT.\n",
        "\n",
        "* El primer cambio lo hacemos en la fase de Tokenizacion, ya que necesitamos añadir algunos tokens a las entradas de BERT. El primero es CLS el cual va al inicio, se usa para probelams de clasificacion, y un token SEP de separacion entre frases.\n",
        "Para la funcion encode_sentence, esta ahora devolvera el token CLS al inicio, luego la tokenizacion de la frase y por ultimo  el separador SEP.\n",
        "\n",
        "* El segundo cambio lo hacemos en la seccion \"Creacion de dataset\". La capa de BERT necesita tres entredas, formadas por tres tipos de tokens diferentes, y estos tokens se generan a partir de las frases (elementos tokenizados). Entonces, una vez que tenemos la frase tokenizada, necesitamos crear esos tres tokens, y para ello necesitaremos dos cosas. La primera es una lista de mascaras. Esto le va a indicar a BERT cuales son los tokens que no tiene que utilizar en la transformacion de datos para convertirlos en vectores de embedding, porque son valores desconocidos que tiene que predecir.  Si uno de los elementos de la frase esta marcado por la mascara, entonces ese sera el valor que queremos predecir (no importa cuales son sus coordenadas, ya que son desconocidas, son las que se quieren predecir)\n",
        "No hemos usado padding manual todavia.\n",
        "Tambien hara falta saber quien pertenece a la primera frase y quien pertenece a la segunda frase. El tercer tipo de enetrada era el segment input, que basicamente era una lista de ceros y unos, indicando si era la primera frase, en cuyo caso los tokens seran los ceros.\n",
        "\n",
        "Entonces la primera funcion, get_ids, la cual, a partir de una lista de tokens de palabras tokenizadas, palabras separadas, mas el CLS y mas el separador SEP, el tokenizer se encargara de convertir los tokens a ids. Con esto devolvemos la lista de ids para cada uno de los tokens, la version del formto numero en lugar de la version en formato string que habiamos dejado lista a partir del resultado de la linea anterior.\n",
        "\n",
        "La segunda funcion (get_mask) es la de aplicar la mascara correspondiente a los tokens de padding. En esta funcion lo que se hara sera buscar, dentro de la lista de tokens, la existencia del token PAD. Cuando encontremos este token, significa que ese elemento no nos interesa. La funcion busca los tokens que no son iguales a PAD, devolviendo valores 1 para los que no son PAD y 0s para los que si son PAD. Basicamente esta funcion devolvera una lista de la misma longitud que la de tokens que le haya pasado como entrada, pero con ceros y unos.\n",
        "\n",
        "Y la tercera funcion indicara si un token pertenece a la primera frase o a la segunda frase. Esto lo haresmos gracias a ubicar el token de separacion en cuestion, ya que hasta encontrar el token SEP, se utilizaran ceros para indicar que el fragmento de frase corresponde al primer trozo, y despues de encontrar el token de separacion, colocaremos unos en este caso.\n",
        "\n",
        "Ahora hay que cambiar es sorted_all, la cual ahora generara vectores de 4 alementos para cada fila, y cada uno es una tupla. En primer lugar se ponen los identificadores para la frase (sentence label(sent_lab) es el iterador de data_with_len, por tanto, en sentence label nos encontramos con elementos que tienen tres columnas, en la cero esta la frase, en la uno esta la etiqueta y en la dos esta la longitud). Luego se le pone la mascara (get_mask) para la misma frase y los segmentos (get_segment) para la propia frase. De este modo empaquetamos en la tupla, en primera posicion ([0]) esos tres elementos (identificadores, mascaras y segmentos) en una lista, y en la segunda ([1])posicion de la tupla la etiqueta, por lo que sent_lab[1] nos dara la etiqueta para esa frase que acabo de triplicar su tamaño, gracias a los identificadores, tokens, la mascara y los segmentos de frase 01. Cabe mensionar que los ids, las mascaras y los segmentos entraran en el embedding de BERT junto con la etiqueta.\n",
        "\n",
        "Llegado a este punto, ya se hicieron todos los cambios necesarios en la entrada para poder utilizar BERT como capa de embedding en nuestra red neuronal convolucional, por lo que la segunda fase esta lista.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-avmI4Yb779-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La capa de embedding de BERT\n",
        "Lo que sigue es modificar el modelo (construccion del modelo) y evaluar su performance para ver que tanto mejor se comporta con respecto a la capa de medir previa que habiamos creado utilizando tensorflow. \n",
        "\n",
        "Cuando aplicaquemos la bert.layer, veremos que salen dos tersones. El primero tiene dimension (1,768), esto significa que es un lote de una sola frase, y esa frase contiene 768 elementos, esa es la dimension del embedding, y esto se corresponde al token CLS. Esta es la informacion que se utilizaria en tareas de clasificacion (representacion vectorial de una frase).\n",
        "El segundo tensor es de dimension (1, 6, 768), el cual es de un solo lote, pero ahora tiene coordenadas de 768 valores para cada token, osea, para cada palabra.\n",
        "Este tensor no corresponde a la codificacion de toda la frase, sino, con la codificacion de todos y cada uno de los elementos de las palabras tokenizadas de nuestra frase. Y 1,6,768 , 1 corresponde al lote, 6 corresponde a las palabras del ejemplo (CLS the roses are red SEP), y 768 corresponde a las coordenadas que mejor se ajustan a cada una de ellas (un vector para cada token). La segunda se usa cuando se necesita llegar a la especificacion del token o palabras, por separado, o sea, una aplicacion de palabra a palabra (de token a token, o sea, la representacion vectorial de cada una de las palabras).\n",
        "#### EJEMPLO\n",
        "\n",
        "my_sent = [\"[CLS]\"] + tokenizer.tokenize(\"Roses are red.\") + [\"[SEP]\"]\n",
        "print(my_sent)\n",
        "\n",
        "bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0)])"
      ],
      "metadata": {
        "id": "Tz8S35IwOMzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cambio de la capa de embedding en el modelo\n",
        "\n",
        "Primero cambiamos el nombre de la clase DCNNBERTEmbedding, y como parametro, ya no esta el tamaño del vocabulario ni la dimension del embedding, ya que la capa de bert ya sabe cual es la dimension del embedding, y tambien ya sabe cual es el tamaño del vocabulario porque utiliza su propio tokenizador.\n",
        "\n",
        "Otro cambio se produjo en la capa de embeddings (self.bart_layer = hub.KerasLeyer), ya que ya no es la de tensorflow, en vez de eso, ahora se usa algo parecido a como se uso la primera capa de bert para el tokenizador. Tambien se usa el trainable = false, ya que se quiere utilizar el embedding ya entrenado por google (si fuera true se haria un finetunning).\n",
        "\n",
        "Otro cambio se produce en la funcion call, ya que ahora llama a otra funcion nueva llamada embed_with_bert, la cual recibe el parametro all_tokens (token de entrada. Recordar que vamos a utilizar bloques (batches) de tokens), y lo que se hace es, usando la capa ver definida en init (__init__) para darle la forma correcta, ya que, para cada grupo de tokens, se deben obtener los identificacdores, la mascara y el token de frase en el orden adecuado. Entonces basicamente lo que se hace es entrar a all_tokens, se construye en una lista de python accediendo, de todos los tokens, a todo el bloque (a todas las frases que vengan en el bloque). El elemento 0 (all_tokens[:,0,:]) son los token que correspode a las palabras de cada frase. El elemento 1 (all_tokens[:,1,:]), que es el de tokens de mascara, quiere decir que de todo el bloque obtenemos unicamenten la dimension que corresponde a la cascara y para todas y cada una de las palabras. Y lo mismo sucede con el token de frase, o sea, con el 2 (all_tokens[:,2,:]). \n",
        "\n",
        " def embed_with_bert(self, all_tokens):\n",
        "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
        "                                   all_tokens[:, 1, :],\n",
        "                                   all_tokens[:, 2, :]])\n",
        "        return embs\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embed_with_bert(inputs)\n",
        "\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "Una vez que tenemos la forma, construimos la entrada, tal como lo hemos visto hates (ejemplo capa de embedding).\n",
        "Recordar que nos da una salida doble (_,embs). La primera respuesta (_) se usa cuando se quiere una analisis de la frase en  global (vector de 768 que representa la frase en todo si contexto), y la segudas parte es la que se corresponde con los tokens, las palabras individuales de mapeados a en dimension 768. En el primero ponemos _ porque no nos aporta nada, ya que lo que se quiere es analizar palabra por palabra y por tanto se retornan los embeddings que es el segundo de los tensores que devuelven la capa de bert de la capa de embedding. "
      ],
      "metadata": {
        "id": "7rk63Ti4S1pI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento \n",
        "\n",
        "En esta fase no cambia casi nada, solo en la parte donde se llama a la red neuronal, ya que no necesitamos la dimension del vocabulario ni la dimension del embedding, por lo que se debe enviar menos informacion.\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "\n",
        "                        \n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5\n",
        "\n",
        "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
        "                         FFN_units=FFN_UNITS,\n",
        "                         nb_classes=NB_CLASSES,\n",
        "                         dropout_rate=DROPOUT_RATE)\n",
        "```\n",
        "\n",
        "## Resumen\n",
        "\n",
        "* La entrada de BERT requiere de una entrada triple de (get_ids, get_mask, get_segments).\n",
        "\n",
        "* Cuando se llama a la capa de embedding se obtienen dos respuestas diferentes (_, embs), un vector que representa la frase ene su totalidad y otro vector que \n",
        "es una lista de vectores, cada uno de los cuales representa cada una de las palabras de los tokens utilizados (para nuestro caso usamos la segunda).\n",
        "\n",
        "* Si se quiere usar BERT como herramienta se debe recordar poner la false en trainable.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=False)\n",
        "```\n",
        "* Y una observacion es que al final, en la fase de entrenamiento, prescindimos de hiperaparametros, por lo que solo se usan unos pocos.\n"
      ],
      "metadata": {
        "id": "kpnEv8IQZocg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUU4TlmoFMZ_"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import time\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXj8lk3uGn4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99665a65-8d54-446d-9144-361cb31b4f1c"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 177 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=73ef72a59297643242a77c26a710ba7e1b93dd47f0f2e80553e22850146f49d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=45eba42ce2d3d154f1d4f31e8add784107f45c191fd34ec6e03cd7154b2d9117\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=de67dad60dc2a1b9de6b8dea3731dfae03f91b869afbfdb5dc0fd2d07aeaaf5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 27.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOfuPdFHFpfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a4b517-8664-4208-8050-5e8b2f49bac3"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ZbE2lPDIFL"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9S77lewDNE1"
      },
      "source": [
        "## Carga de los ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7GET0xsDSDc"
      },
      "source": [
        "Cargamos los ficheros de nuestro Google Drive personal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hABc0h8GdTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0064db85-49d4-431d-916f-ebb53fb949d6"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slnILsqwGxTX"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/Colab_Notebooks/Bert_udemy/Aplicacion_dos_embedding_BERT/training.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REdK4z4YG9kZ"
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2g61evDZb4"
      },
      "source": [
        "## Pre Procesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCyy4babDrI8"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEyorQS_HArn"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BlbZpy0HHiV"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6SOj46BHKEk"
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasar los datos filtrados a un text"
      ],
      "metadata": {
        "id": "15J1w71LzHhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(15):\n",
        "    algo = f\"{data_labels[i]}, {data_clean[i]}\"\n",
        "    print(algo)"
      ],
      "metadata": {
        "id": "y3NrHSztz0ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_labels))\n",
        "print(len(data_clean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IONrqusj2l-a",
        "outputId": "a23f3f98-b312-457c-a38c-0f0c0b39afca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600000\n",
            "1600000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "archivo = open(\"/content/drive/MyDrive/Colab_Notebooks/Bert_udemy/Aplicacion_dos_embedding_BERT/data_clean.txt\",'w')\n",
        "for i in range(1600000):\n",
        "    algo = f\"{data_labels[i]}, {data_clean[i]}\"\n",
        "    archivo.write(algo)\n",
        "    archivo.write(\"\\n\")\n",
        "    \n",
        "archivo.close()"
      ],
      "metadata": {
        "id": "dmL5QSuKzF1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJa3YWeJD1gM"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUaCPqqBD7kQ"
      },
      "source": [
        "Necesitamos crear una capa BERT para tener acceso a los metadatos del tokenizador (como el tamaño del vocabulario)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wry-st-HMN0"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMVarTJpELyK"
      },
      "source": [
        "Solo usamos la primera oración para las entradas BERT, por lo que agregamos el token CLS al principio y el token SEP al final de cada oración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-JkZt9NduoC"
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_clean[1:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l-KAtDsk1H-",
        "outputId": "1414bbd8-610f-4775-d930-6c46300bcf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!\", ' I dived many times for the ball. Managed to save The rest go out of bounds']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pel_Uk6Ic4xB"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32MeEwnkCB8"
      },
      "source": [
        "### Creación del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUVc83VNEcW9"
      },
      "source": [
        "Necesitamos crear las 3 entradas diferentes para cada oración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmW9JZLJaxww"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # convierte los 1 en 0 y vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x06fFPFtFqVK"
      },
      "source": [
        "Crearemos padded batches (por lo que rellenamos las frases para cada lote de forma independiente), de esta forma añadimos el mínimo número de tokens de padding posible. Para eso, ordenamos las frases por longitud, aplicamos padded_batches y luego las mezclamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjAVGCwlb6F8"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [([get_ids(sent_lab[0]),\n",
        "                get_mask(sent_lab[0]),\n",
        "                get_segments(sent_lab[0])],\n",
        "               sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkMiqmzsfo6a"
      },
      "source": [
        "# Una lista es un tipo de iterador de modo que se puede usar como generador de un dataset\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkGWlzeOfos6"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
        "                                       padded_shapes=((3, None), ()),\n",
        "                                       padding_values=(0, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aA7it--hHl4"
      },
      "source": [
        "NB_BATCHES = math.ceil((len(sorted_all) / BATCH_SIZE)*0.288)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(NB_BATCHES )"
      ],
      "metadata": {
        "id": "Aqegavbs0ki1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4QCPok7aEM_"
      },
      "source": [
        "next(iter(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbR6oHocxZvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504c093f-293c-49be-9153-208f43c0ab91"
      },
      "source": [
        "my_sent = [\"[CLS]\"] + tokenizer.tokenize(\"Roses are red.\") + [\"[SEP]\"]\n",
        "print(my_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'roses', 'are', 'red', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uVFx3Sxzva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ac655e-6aa9-4bcb-ef86-2cfd9f57dc79"
      },
      "source": [
        "bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
              " array([[-9.27935600e-01, -4.10335362e-01, -9.65754867e-01,\n",
              "          9.07317877e-01,  8.12913775e-01, -1.74174309e-01,\n",
              "          9.11234498e-01,  3.41952324e-01, -8.74521255e-01,\n",
              "         -9.99989271e-01, -7.78410017e-01,  9.69385266e-01,\n",
              "          9.86160576e-01,  6.36963069e-01,  9.48631287e-01,\n",
              "         -7.51193106e-01, -4.58339393e-01, -7.08104432e-01,\n",
              "          4.62098300e-01, -6.57927155e-01,  7.60414660e-01,\n",
              "          9.99994814e-01, -3.96860808e-01,  3.44166070e-01,\n",
              "          6.16488814e-01,  9.94400024e-01, -7.76633799e-01,\n",
              "          9.38316524e-01,  9.59452271e-01,  7.32879460e-01,\n",
              "         -6.93436682e-01,  2.93080449e-01, -9.93785560e-01,\n",
              "         -1.64551765e-01, -9.67019558e-01, -9.95549619e-01,\n",
              "          5.32935560e-01, -6.88061118e-01,  1.34715429e-02,\n",
              "          2.98195258e-02, -9.18356538e-01,  4.20526177e-01,\n",
              "          9.99988973e-01,  2.52676368e-01,  6.06235564e-01,\n",
              "         -3.50750148e-01, -1.00000000e+00,  4.97585416e-01,\n",
              "         -8.95187378e-01,  9.62560952e-01,  9.43730652e-01,\n",
              "          9.03285563e-01,  1.54699728e-01,  5.86143732e-01,\n",
              "          5.80860496e-01, -4.05053079e-01, -2.76641771e-02,\n",
              "          2.98045963e-01, -2.83075958e-01, -6.47424459e-01,\n",
              "         -6.51523829e-01,  5.43847382e-01, -9.56302047e-01,\n",
              "         -9.22750413e-01,  9.61462915e-01,  8.27475190e-01,\n",
              "         -3.50112528e-01, -4.06405658e-01, -8.74317735e-02,\n",
              "         -9.98740569e-02,  8.96688223e-01,  3.00931633e-01,\n",
              "         -1.51129290e-01, -8.52713525e-01,  8.09592366e-01,\n",
              "          4.00989205e-01, -6.61606014e-01,  1.00000000e+00,\n",
              "         -6.16246402e-01, -9.86407220e-01,  8.90942991e-01,\n",
              "          8.11157644e-01,  5.81394613e-01, -6.33872926e-01,\n",
              "          3.78197849e-01, -1.00000000e+00,  6.76351368e-01,\n",
              "         -2.30612651e-01, -9.92552578e-01,  3.85461122e-01,\n",
              "          6.57650828e-01, -2.90105790e-01,  4.46832448e-01,\n",
              "          6.28524125e-01, -5.58409274e-01, -6.65295124e-01,\n",
              "         -4.72272277e-01, -9.28039372e-01, -3.54472518e-01,\n",
              "         -6.19735956e-01,  1.24534965e-01, -3.48905683e-01,\n",
              "         -4.23184186e-01, -4.20834780e-01,  4.56588715e-01,\n",
              "         -6.14470959e-01, -5.15243173e-01,  5.01909971e-01,\n",
              "          4.29147243e-01,  7.59821951e-01,  4.37516510e-01,\n",
              "         -4.33598369e-01,  6.30962014e-01, -9.59743083e-01,\n",
              "          7.73877442e-01, -3.95738035e-01, -9.87354517e-01,\n",
              "         -6.73180103e-01, -9.92996454e-01,  7.77800262e-01,\n",
              "         -5.05856276e-01, -3.19990993e-01,  9.69388783e-01,\n",
              "         -3.51620406e-01,  3.79092008e-01, -2.21649304e-01,\n",
              "         -9.51505601e-01, -1.00000000e+00, -8.80426824e-01,\n",
              "         -8.34713042e-01, -2.77321547e-01, -4.70461339e-01,\n",
              "         -9.83711898e-01, -9.56730306e-01,  6.61120892e-01,\n",
              "          9.56025839e-01,  1.62189081e-01,  9.99961317e-01,\n",
              "         -5.11205912e-01,  9.59530592e-01, -5.58609903e-01,\n",
              "         -8.00221145e-01,  8.48543882e-01, -5.58320582e-01,\n",
              "          8.33738446e-01,  2.63149440e-01, -7.33846724e-01,\n",
              "          3.16189617e-01, -4.83306348e-01,  6.87450051e-01,\n",
              "         -7.94889867e-01, -3.81298304e-01, -8.71705949e-01,\n",
              "         -9.49488103e-01, -3.62460166e-01,  9.51175630e-01,\n",
              "         -7.62520671e-01, -9.61278081e-01, -1.53293774e-01,\n",
              "         -4.02463555e-01, -5.69815516e-01,  8.52475762e-01,\n",
              "          7.99817979e-01,  5.33586383e-01, -6.96546555e-01,\n",
              "          4.84272510e-01,  2.24440649e-01,  7.31195331e-01,\n",
              "         -8.18207979e-01, -3.58149379e-01,  5.32028556e-01,\n",
              "         -4.41676080e-01, -9.25720930e-01, -9.87607241e-01,\n",
              "         -5.07007718e-01,  5.31485558e-01,  9.93827164e-01,\n",
              "          7.66175270e-01,  4.12393093e-01,  8.83270144e-01,\n",
              "         -3.85666490e-01,  8.81850183e-01, -9.67345178e-01,\n",
              "          9.86407518e-01, -3.13535541e-01,  3.57363909e-01,\n",
              "         -6.57590449e-01,  2.70097494e-01, -8.59112978e-01,\n",
              "          2.32003823e-01,  8.62853050e-01, -9.03662384e-01,\n",
              "         -7.94610143e-01, -2.82699227e-01, -4.76583838e-01,\n",
              "         -5.01097441e-01, -8.80422354e-01,  5.45586228e-01,\n",
              "         -4.41977412e-01, -5.77729046e-01, -1.25810012e-01,\n",
              "          9.06032026e-01,  9.80562270e-01,  8.44253480e-01,\n",
              "          5.20119548e-01,  7.80579507e-01, -9.23414290e-01,\n",
              "         -5.87243795e-01,  2.34754518e-01,  2.97746927e-01,\n",
              "          3.54463756e-01,  9.96218681e-01, -8.01237464e-01,\n",
              "         -2.79998302e-01, -9.39948916e-01, -9.83751893e-01,\n",
              "          3.82993594e-02, -9.28821862e-01, -2.94137955e-01,\n",
              "         -7.00600803e-01,  7.67863870e-01, -3.51922095e-01,\n",
              "          6.57683909e-01,  5.54107010e-01, -9.90459323e-01,\n",
              "         -7.80434251e-01,  5.50272882e-01, -5.03933012e-01,\n",
              "          5.56852520e-01, -3.53224486e-01,  7.86349893e-01,\n",
              "          9.69607174e-01, -6.54102087e-01,  7.34232843e-01,\n",
              "          8.81996751e-01, -9.14772034e-01, -7.82534778e-01,\n",
              "          8.52741361e-01, -4.38667655e-01,  8.24172258e-01,\n",
              "         -7.77354538e-01,  9.91627097e-01,  9.48048532e-01,\n",
              "          7.74401665e-01, -9.52811956e-01, -7.52400279e-01,\n",
              "         -8.65391135e-01, -8.10665190e-01, -1.91085428e-01,\n",
              "          6.22541942e-02,  9.39342856e-01,  6.60155118e-01,\n",
              "          5.10393083e-01,  3.03855419e-01, -7.58786082e-01,\n",
              "          9.97947276e-01, -8.39390635e-01, -9.73821759e-01,\n",
              "         -6.96808159e-01, -4.71226066e-01, -9.92139876e-01,\n",
              "          9.27336812e-01,  3.11118245e-01,  6.17148519e-01,\n",
              "         -5.93245447e-01, -7.30744123e-01, -9.74081159e-01,\n",
              "          9.14184034e-01,  2.35107005e-01,  9.90232766e-01,\n",
              "         -4.98075664e-01, -9.59739566e-01, -7.62371123e-01,\n",
              "         -9.30913270e-01, -4.32067700e-02, -2.13116273e-01,\n",
              "         -6.06085181e-01, -2.81608757e-02, -9.69718456e-01,\n",
              "          6.36244357e-01,  6.35316491e-01,  5.37905633e-01,\n",
              "         -8.91036212e-01,  9.99303102e-01,  1.00000000e+00,\n",
              "          9.73003805e-01,  9.01396930e-01,  8.87466550e-01,\n",
              "         -9.99958634e-01, -6.90021813e-01,  9.99998033e-01,\n",
              "         -9.93730605e-01, -1.00000000e+00, -9.37510788e-01,\n",
              "         -8.12253237e-01,  2.70661086e-01, -1.00000000e+00,\n",
              "         -2.87079155e-01, -1.50920197e-01, -9.31140363e-01,\n",
              "          8.18554997e-01,  9.78328943e-01,  9.94965494e-01,\n",
              "         -1.00000000e+00,  8.81453693e-01,  9.30843115e-01,\n",
              "         -7.06026852e-01,  9.76767123e-01, -6.08330011e-01,\n",
              "          9.75543499e-01,  5.93019426e-01,  5.54319263e-01,\n",
              "         -2.44307309e-01,  4.22843546e-01, -9.68066275e-01,\n",
              "         -9.14158463e-01, -7.75702655e-01, -7.79753387e-01,\n",
              "          9.98873472e-01,  2.67525733e-01, -7.70681083e-01,\n",
              "         -9.30970490e-01,  6.98258400e-01, -1.79436088e-01,\n",
              "          1.48644865e-01, -9.69404399e-01, -3.27199668e-01,\n",
              "          7.69222617e-01,  8.38437557e-01,  2.74363279e-01,\n",
              "          4.46673691e-01, -6.88233912e-01,  4.38525409e-01,\n",
              "         -6.96207508e-02,  2.83885270e-01,  6.96684957e-01,\n",
              "         -9.55272675e-01, -5.49684405e-01, -3.89561355e-01,\n",
              "          3.75222683e-01, -7.64762044e-01, -9.54123020e-01,\n",
              "          9.69721198e-01, -4.86066669e-01,  9.72205639e-01,\n",
              "          1.00000000e+00,  7.64813483e-01, -9.13303673e-01,\n",
              "          6.57082438e-01,  4.31852072e-01, -7.01079428e-01,\n",
              "          1.00000000e+00,  8.67337167e-01, -9.83669579e-01,\n",
              "         -5.84471881e-01,  7.79540896e-01, -6.77890062e-01,\n",
              "         -7.74523735e-01,  9.99660969e-01, -3.41093063e-01,\n",
              "         -8.14479828e-01, -6.48069203e-01,  9.86273050e-01,\n",
              "         -9.94089186e-01,  9.97642994e-01, -8.94537747e-01,\n",
              "         -9.79997158e-01,  9.60477769e-01,  9.49231923e-01,\n",
              "         -6.83828175e-01, -7.17898607e-01,  2.86706805e-01,\n",
              "         -7.60040939e-01,  4.78332579e-01, -9.51963305e-01,\n",
              "          8.08321297e-01,  5.27614057e-01, -1.67665854e-01,\n",
              "          9.16268110e-01, -8.87898982e-01, -5.93430936e-01,\n",
              "          3.90308022e-01, -7.76923299e-01, -3.84818435e-01,\n",
              "          9.59038138e-01,  6.78381324e-01, -4.08702821e-01,\n",
              "         -1.99679174e-02, -4.68428731e-01, -7.41143048e-01,\n",
              "         -9.73734379e-01,  6.23253882e-01,  1.00000000e+00,\n",
              "         -4.31855381e-01,  8.94348621e-01, -5.72569311e-01,\n",
              "         -1.89501010e-02,  7.24834725e-02,  6.05421424e-01,\n",
              "          5.64564049e-01, -5.04035056e-01, -8.33653033e-01,\n",
              "          9.20378506e-01, -9.70664859e-01, -9.92627263e-01,\n",
              "          8.63119662e-01,  2.32818514e-01, -3.05338442e-01,\n",
              "          9.99999225e-01,  6.51024520e-01,  3.69558871e-01,\n",
              "          5.16951740e-01,  9.89937425e-01, -5.10574579e-02,\n",
              "          5.19780755e-01,  9.13519502e-01,  9.89344239e-01,\n",
              "         -4.06514227e-01,  6.72227561e-01,  8.66246223e-01,\n",
              "         -9.63320732e-01, -3.93905371e-01, -7.32534409e-01,\n",
              "          6.66500553e-02, -9.50429082e-01,  5.36764003e-02,\n",
              "         -9.64523733e-01,  9.78591204e-01,  9.72525179e-01,\n",
              "          5.02412796e-01,  3.42612624e-01,  8.20066929e-01,\n",
              "          1.00000000e+00, -8.37067664e-01,  5.97411454e-01,\n",
              "         -4.17201877e-01,  8.81286025e-01, -9.99911070e-01,\n",
              "         -8.37778270e-01, -4.66962278e-01, -2.72496670e-01,\n",
              "         -9.03814435e-01, -4.58637834e-01,  3.91833574e-01,\n",
              "         -9.79059279e-01,  9.10196245e-01,  8.29555333e-01,\n",
              "         -9.92893636e-01, -9.93933380e-01, -5.58821261e-01,\n",
              "          7.86012113e-01,  2.98600823e-01, -9.94314432e-01,\n",
              "         -8.16725373e-01, -6.58432007e-01,  9.07821834e-01,\n",
              "         -4.84595984e-01, -9.59578753e-01, -5.24700582e-01,\n",
              "         -4.26523268e-01,  5.39447308e-01, -3.51429641e-01,\n",
              "          6.03987992e-01,  8.84236515e-01,  6.91960514e-01,\n",
              "         -7.73553610e-01, -3.49986732e-01, -1.82106078e-01,\n",
              "         -8.09592664e-01,  9.06841516e-01, -8.09706271e-01,\n",
              "         -9.76247668e-01, -2.70705462e-01,  1.00000000e+00,\n",
              "         -5.54332614e-01,  8.93760264e-01,  7.55229652e-01,\n",
              "          7.80316293e-01, -1.99225381e-01,  3.35151255e-01,\n",
              "          9.55944061e-01,  3.82269740e-01, -7.57196724e-01,\n",
              "         -9.39319909e-01, -6.35581613e-01, -6.07329130e-01,\n",
              "          7.00571835e-01,  7.23613143e-01,  7.29010999e-01,\n",
              "          8.65883768e-01,  7.64537394e-01,  2.08820984e-01,\n",
              "         -6.98528215e-02, -5.64191781e-04,  9.99799311e-01,\n",
              "         -4.44100171e-01, -1.80671468e-01, -4.89859760e-01,\n",
              "         -2.91431367e-01, -4.25409168e-01, -1.98749691e-01,\n",
              "          1.00000000e+00,  3.56602192e-01,  7.75661469e-01,\n",
              "         -9.93823886e-01, -9.28070962e-01, -9.31738496e-01,\n",
              "          1.00000000e+00,  8.50040436e-01, -7.60715842e-01,\n",
              "          7.18036532e-01,  7.75469124e-01, -1.75161973e-01,\n",
              "          8.09469283e-01, -3.36547822e-01, -3.02385211e-01,\n",
              "          4.57467943e-01,  3.08043748e-01,  9.70232069e-01,\n",
              "         -6.18903935e-01, -9.75721240e-01, -5.94948888e-01,\n",
              "          5.63390791e-01, -9.66651082e-01,  9.99981165e-01,\n",
              "         -6.10340416e-01, -3.60574961e-01, -4.96435732e-01,\n",
              "         -4.91436243e-01,  4.47817594e-01,  2.87387948e-02,\n",
              "         -9.83154833e-01, -3.47387344e-01,  3.09110671e-01,\n",
              "          9.66638982e-01,  3.75864029e-01, -6.41106606e-01,\n",
              "         -8.90264869e-01,  8.92269194e-01,  8.32000017e-01,\n",
              "         -9.59132075e-01, -9.57766414e-01,  9.71166372e-01,\n",
              "         -9.84971106e-01,  7.67819285e-01,  1.00000000e+00,\n",
              "          3.83998871e-01,  4.38051224e-01,  3.52292389e-01,\n",
              "         -4.46136445e-01,  4.46569413e-01, -6.90631509e-01,\n",
              "          6.74425542e-01, -9.59155858e-01, -4.53285009e-01,\n",
              "         -2.96152592e-01,  3.57684523e-01, -2.41154611e-01,\n",
              "         -5.88313818e-01,  7.63308346e-01,  3.13667506e-01,\n",
              "         -6.03100598e-01, -6.84795618e-01, -2.60147184e-01,\n",
              "          5.75160444e-01,  9.16844189e-01, -3.56800199e-01,\n",
              "         -2.31557846e-01,  1.15728118e-01, -1.77119032e-01,\n",
              "         -9.47563529e-01, -5.23142159e-01, -6.04617894e-01,\n",
              "         -9.99998510e-01,  5.41667759e-01, -1.00000000e+00,\n",
              "          6.60002053e-01,  3.39036524e-01, -2.57962316e-01,\n",
              "          8.98434103e-01,  3.58503610e-01,  7.80092001e-01,\n",
              "         -8.63456190e-01, -9.04243648e-01,  2.35174313e-01,\n",
              "          8.47542286e-01, -4.83705014e-01, -7.76437223e-01,\n",
              "         -7.77087033e-01,  4.51546967e-01, -1.20644003e-01,\n",
              "          3.45337838e-01, -7.58304536e-01,  7.38663554e-01,\n",
              "         -2.54878402e-01,  1.00000000e+00,  1.56726688e-01,\n",
              "         -6.47172868e-01, -9.80846465e-01,  3.21544886e-01,\n",
              "         -3.49480033e-01,  1.00000000e+00, -8.88086200e-01,\n",
              "         -9.70758736e-01,  4.17613924e-01, -6.59506381e-01,\n",
              "         -8.39061737e-01,  4.56446022e-01,  7.08391592e-02,\n",
              "         -8.59648883e-01, -9.68725741e-01,  9.56583977e-01,\n",
              "          8.95311058e-01, -6.79162502e-01,  7.91996121e-01,\n",
              "         -3.77204865e-01, -5.99682450e-01,  1.89219400e-01,\n",
              "          9.34770346e-01,  9.87944603e-01,  7.07965136e-01,\n",
              "          9.21087861e-01, -1.59540206e-01, -4.83467519e-01,\n",
              "          9.76640403e-01,  2.95252264e-01,  5.32053411e-01,\n",
              "          3.22658211e-01,  1.00000000e+00,  4.97991741e-01,\n",
              "         -9.31000948e-01, -3.24744463e-01, -9.82841551e-01,\n",
              "         -2.67996371e-01, -9.52166140e-01,  4.53916818e-01,\n",
              "          3.94372314e-01,  9.26190197e-01, -3.09752166e-01,\n",
              "          9.69366550e-01, -9.40263689e-01,  1.66798458e-01,\n",
              "         -8.32233012e-01, -7.04411030e-01,  5.49370527e-01,\n",
              "         -9.30373490e-01, -9.88702476e-01, -9.91572320e-01,\n",
              "          7.38682628e-01, -5.26327372e-01, -9.29533243e-02,\n",
              "          2.77956098e-01,  2.54385620e-01,  5.55892944e-01,\n",
              "          5.70780337e-01, -1.00000000e+00,  9.51999605e-01,\n",
              "          5.82980871e-01,  9.13394034e-01,  9.78624403e-01,\n",
              "          7.49032199e-01,  7.39971817e-01,  3.71186584e-01,\n",
              "         -9.89660740e-01, -9.84848499e-01, -5.31397820e-01,\n",
              "         -3.88979614e-01,  8.49413812e-01,  8.17017078e-01,\n",
              "          8.92696917e-01,  6.16892219e-01, -5.75284719e-01,\n",
              "         -2.86467373e-01, -7.60570645e-01, -7.78939366e-01,\n",
              "         -9.94441748e-01,  5.72188258e-01, -7.72194862e-01,\n",
              "         -9.57696319e-01,  9.67420697e-01, -2.17978925e-01,\n",
              "         -1.75552785e-01, -3.26556891e-01, -9.06777918e-01,\n",
              "          9.35597479e-01,  7.66239524e-01,  1.90596223e-01,\n",
              "          1.53928921e-01,  5.40217698e-01,  9.02483523e-01,\n",
              "          9.40388858e-01,  9.88884807e-01, -9.10143971e-01,\n",
              "          7.88359940e-01, -8.31383049e-01,  6.11195445e-01,\n",
              "          8.21669102e-01, -9.41967726e-01,  3.75133276e-01,\n",
              "          5.49405813e-01, -6.15318477e-01,  3.91501933e-01,\n",
              "         -3.66627246e-01, -9.74752426e-01,  8.88878345e-01,\n",
              "         -3.62838209e-01,  6.53205693e-01, -5.35070002e-01,\n",
              "         -2.21281648e-02, -4.40158099e-01, -3.87567371e-01,\n",
              "         -7.89354980e-01, -6.70902491e-01,  6.87369943e-01,\n",
              "          4.34680045e-01,  9.07510877e-01,  9.13954079e-01,\n",
              "         -1.07544959e-01, -8.54991913e-01, -3.22965920e-01,\n",
              "         -7.80992568e-01, -9.35075521e-01,  9.56621051e-01,\n",
              "         -2.46967241e-01, -1.84675857e-01,  7.18141854e-01,\n",
              "          1.66834041e-01,  9.54272568e-01,  5.20279408e-01,\n",
              "         -5.11346459e-01, -3.58430535e-01, -7.76734531e-01,\n",
              "          9.01378989e-01, -6.45810366e-01, -6.68203235e-01,\n",
              "         -6.79575741e-01,  8.30889285e-01,  4.56940055e-01,\n",
              "          9.99998212e-01, -8.61587346e-01, -9.52708542e-01,\n",
              "         -5.74054837e-01, -4.75623369e-01,  5.11585891e-01,\n",
              "         -7.02607393e-01, -1.00000000e+00,  5.05624473e-01,\n",
              "         -6.52045965e-01,  8.13730776e-01, -8.72139215e-01,\n",
              "          8.09319794e-01, -8.18222106e-01, -9.88196373e-01,\n",
              "         -4.00082499e-01,  3.38945836e-01,  7.67013788e-01,\n",
              "         -5.16352713e-01, -8.75940859e-01,  6.15952253e-01,\n",
              "         -7.52709746e-01,  9.89328444e-01,  8.95710647e-01,\n",
              "         -6.14250362e-01,  2.19650120e-01,  7.52754331e-01,\n",
              "         -8.20389688e-01, -8.05691302e-01,  9.38039362e-01]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 6, 768), dtype=float32, numpy=\n",
              " array([[[-0.07947499,  0.00580769, -0.3141406 , ..., -0.4509728 ,\n",
              "           0.2933317 ,  0.23387675],\n",
              "         [ 0.3931601 ,  0.50336283,  0.24021375, ..., -0.32635653,\n",
              "           0.34986085,  0.20673229],\n",
              "         [ 0.35789192,  0.10767104, -0.04988898, ..., -0.5082274 ,\n",
              "           0.25048807, -0.26268804],\n",
              "         [-0.2989219 , -0.24708785,  0.07151502, ..., -0.33810055,\n",
              "           0.12699498, -0.09681895],\n",
              "         [-0.36815375, -0.7146524 , -0.21032608, ...,  0.35395208,\n",
              "           0.33438638, -0.62334776],\n",
              "         [ 0.8869221 , -0.1699698 , -0.29173732, ...,  0.05816461,\n",
              "          -0.5775992 , -0.32075307]]], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2pxAPFxGe8r"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ"
      },
      "source": [
        "class DCNNBERTEmbedding(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=False)\n",
        "\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def embed_with_bert(self, all_tokens):\n",
        "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
        "                                   all_tokens[:, 1, :],\n",
        "                                   all_tokens[:, 2, :]])\n",
        "        return embs\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embed_with_bert(inputs)\n",
        "\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsWpzQz2IQvJ"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf"
      },
      "source": [
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HPbZ72KPPnX"
      },
      "source": [
        "\n",
        "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
        "                         FFN_units=FFN_UNITS,\n",
        "                         nb_classes=NB_CLASSES,\n",
        "                         dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpHDseF0QLl3"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1hdT_JT2Rfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9a1948-949d-4d40-ec46-05e8c40713ab"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Colab_Notebooks/Bert_udemy/Aplicacion_dos_embedding_BERT/checkponit\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Último checkpoint restaurado!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8LHztku2cjl"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint guardado en {}.\".format(checkpoint_path))\n",
        "        print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn.build()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "aJXY9Gefz0sW",
        "outputId": "769b590d-912e-4319-d2c1-7a35caa81d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-941528ad9a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: build() missing 1 required positional argument: 'input_shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Dcnn.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "kloss1sxwYOl",
        "outputId": "3c0ddb0a-1da7-4aeb-f037-4b960cc6b82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-432be87f6ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable)\u001b[0m\n\u001b[1;32m   2774\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2776\u001b[0;31m           \u001b[0;34m'This model has not yet been built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2777\u001b[0m           \u001b[0;34m'Build the model first by calling `build()` or by calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m           'the model on a batch of data.')\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0C5lNxFTMrA"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW"
      },
      "source": [
        "start = time.time()\n",
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAb_ijA5Idmz"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQN-Y99WIf6m"
      },
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj98dgxnmhak"
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "\n",
        "    input_ids = get_ids(tokens)\n",
        "    input_mask = get_mask(tokens)\n",
        "    segment_ids = get_segments(tokens)\n",
        "\n",
        "    inputs = tf.stack(\n",
        "        [tf.cast(input_ids, dtype=tf.int32),\n",
        "         tf.cast(input_mask, dtype=tf.int32),\n",
        "         tf.cast(segment_ids, dtype=tf.int32)],\n",
        "         axis=0)\n",
        "    inputs = tf.expand_dims(inputs, 0) # simula un lote\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Negativo.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Positivo.\".format(\n",
        "            output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9jC8UnJgOjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2e4a90-8a6e-4dab-cf18-23fbb215f682"
      },
      "source": [
        "get_prediction(\"i feel strong and good overall\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salida del modelo: [[0.97839725]]\n",
            "Sentimiento predicho: Positivo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aMrBVRbeM29"
      },
      "source": [
        "get_prediction(\"i feel like this was such a rude comment and im glad that t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gardar el modelo"
      ],
      "metadata": {
        "id": "R9qOc-5R9jZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyyaml h5py"
      ],
      "metadata": {
        "id": "Y67z254-0-pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se guarda el modelo\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "Dcnn.save('/content/drive/MyDrive/Colab_Notebooks/Bert_udemy/Aplicacion_dos_embedding_BERT/my_modelo/primer_modelo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0BjFvlXoR-S",
        "outputId": "43f11621-2cea-420c-e7c1-b98ad813fe35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 326). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#cargar el modelo\n",
        "mio_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab_Notebooks/Bert_udemy/Aplicacion_dos_embedding_BERT/my_modelo/primer_modelo')"
      ],
      "metadata": {
        "id": "lY3uHcAl4CtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dcnn.summary()"
      ],
      "metadata": {
        "id": "L-P_nnJt2Aya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0cb142-aaf0-4432-e604-945207ab42a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"dcnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_1 (KerasLayer)  multiple                  109482241 \n",
            "                                                                 \n",
            " conv1d (Conv1D)             multiple                  153700    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           multiple                  230500    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           multiple                  307300    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  multiple                 0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  77056     \n",
            "                                                                 \n",
            " dropout (Dropout)           multiple                  0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110,251,054\n",
            "Trainable params: 768,813\n",
            "Non-trainable params: 109,482,241\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-for-tf2\n",
        "#!pip install sentencepiece"
      ],
      "metadata": {
        "id": "OsgjTbkG6rHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bert"
      ],
      "metadata": {
        "id": "cmqoAOP86Vtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "#from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "TjaEkmZ-7AON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "bhnWpvH86Pe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # convierte los 1 en 0 y vice versa\n",
        "    return seg_ids"
      ],
      "metadata": {
        "id": "EVmsuFOE7kTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "metadata": {
        "id": "q4n2eMcS6J6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictioonn(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "\n",
        "    input_ids = get_ids(tokens)\n",
        "    input_mask = get_mask(tokens)\n",
        "    segment_ids = get_segments(tokens)\n",
        "\n",
        "    inputs = tf.stack(\n",
        "        [tf.cast(input_ids, dtype=tf.int32),\n",
        "         tf.cast(input_mask, dtype=tf.int32),\n",
        "         tf.cast(segment_ids, dtype=tf.int32)],\n",
        "         axis=0)\n",
        "    inputs = tf.expand_dims(inputs, 0) # simula un lote\n",
        "\n",
        "    output = mio_model(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Negativo.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Positivo.\".format(\n",
        "            output))"
      ],
      "metadata": {
        "id": "1ULgpyWq5JPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_predictioonn(\"i feel strong and good overall\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kMIp_Xz5Q1h",
        "outputId": "7ef2f316-f857-401b-e151-dc7096f3a495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salida del modelo: [[0.97839725]]\n",
            "Sentimiento predicho: Positivo.\n"
          ]
        }
      ]
    }
  ]
}